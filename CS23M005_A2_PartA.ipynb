{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOShlJIBHVuEWCEvWOnRHql",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CS23M005/Assignment2_PartA/blob/main/CS23M005_A2_PartA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-IJUyJqmP-B"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "EelbKOQZna8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size, num_filters, filter_size, activation_fun, filter_config):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
        "        self.reul1 = nn.relu()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3)\n",
        "        self.fc1 = nn.Linear(64*4*4, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # N, 3, 32, 32\n",
        "        x = F.relu(self.conv1(x))   # -> N, 32, 30, 30\n",
        "        x = self.pool(x)            # -> N, 32, 15, 15\n",
        "        x = F.relu(self.conv2(x))   # -> N, 64, 13, 13\n",
        "        x = self.pool(x)            # -> N, 64, 6, 6\n",
        "        x = F.relu(self.conv3(x))   # -> N, 64, 4, 4\n",
        "        x = torch.flatten(x, 1)     # -> N, 1024\n",
        "        x = F.relu(self.fc1(x))     # -> N, 64\n",
        "        x = self.fc2(x)             # -> N, 10\n",
        "        return x\n",
        "\n",
        "\n",
        "model = ConvNet().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'[{epoch + 1}] loss: {running_loss / n_total_steps:.3f}')\n",
        "\n",
        "print('Finished Training')\n",
        "PATH = './cnn.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "jOueeD4xuuYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_nn():\n",
        "    config_defaults = {\n",
        "            \"num_epochs\": 3, \\\n",
        "            \"num_hidden_layers\":  4, \\\n",
        "            \"size_hidden_layer\": 128, \\\n",
        "            \"learning_rate\": 1e-3, \\\n",
        "            \"optimizer\": \"rmsprop\", \\\n",
        "            \"batch_size\": 32, \\\n",
        "            \"weight_init\": \"xavier\" , \\\n",
        "            \"activation\": \"tanh\", \\\n",
        "            \"loss\": \"crossentropy\", \\\n",
        "            \"reg_lambda\": 0.001, \\\n",
        "    }\n",
        "    wandb.init(config = config_defaults, project = \"Assignment1\")\n",
        "    config = wandb.config\n",
        "    print(config.optimizer)\n",
        "    wandb.run.name = \"e_{}_hl_{}_opt_{}_bs_{}_init_{}_ac_{}_loss_{}\".format(config.num_epochs,\\\n",
        "                                                                  config.num_hidden_layers,\\\n",
        "                                                                  config.size_hidden_layer,\\\n",
        "                                                                  config.learning_rate,\\\n",
        "                                                                  config.optimizer,\\\n",
        "                                                                  config.batch_size,\\\n",
        "                                                                  config.weight_init,\\\n",
        "                                                                  config.activation,\\\n",
        "                                                                  config.loss,\\\n",
        "                                                                  config.reg_lambda)\n",
        "\n",
        "    seed(42)\n",
        "    n_inputs = 784\n",
        "    n_layers = config.num_hidden_layers\n",
        "    n_neurons = config.size_hidden_layer\n",
        "    n_outputs = 10\n",
        "    init_mode = config.weight_init\n",
        "    w,b = init_nn(n_inputs, n_layers, n_neurons, n_outputs, init_mode)\n",
        "    w_old = w.copy()\n",
        "    b_old = b.copy()\n",
        "\n",
        "    eta = config.learning_rate\n",
        "    activation_f = config.activation\n",
        "    batch_size = config.batch_size\n",
        "    loss = config.loss\n",
        "    ud_lambda = config.reg_lambda\n",
        "    optimizer = config.optimizer\n",
        "\n",
        "    beta = 0.9\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.999\n",
        "\n",
        "    eps = 1e-9\n",
        "    max_epochs = config.num_epochs\n",
        "    momentum = 0.9\n",
        "\n",
        "\n",
        "\n",
        "    wandb.log({\"examples\": fig1})\n",
        "\n",
        "\n",
        "\n",
        "    if optimizer == \"sgd\":\n",
        "      w_n,b_n, a_n, h_n = sgd(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs)\n",
        "    elif optimizer == \"mgd\":\n",
        "      w_n,b_n, a_n, h_n = mgd(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, max_epochs, momentum)\n",
        "    elif optimizer == \"nag\":\n",
        "      w_n,b_n, a_n, h_n = nag(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, momentum)\n",
        "    elif optimizer == \"rmsprop\":\n",
        "      w_n,b_n, a_n, h_n = rmsprop(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, beta, eps)\n",
        "    elif optimizer == \"adam\":\n",
        "      w_n,b_n, a_n, h_n = adam(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, beta1, beta2, eps)\n",
        "    elif optimizer == \"nadam\":\n",
        "      w_n,b_n, a_n, h_n = nadam(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, beta1, beta2, eps)\n",
        "\n",
        "\n",
        "    train_accuracy, yp = accuracy(X_train, y_train, w_n, b_n, n_layers, activation_f)\n",
        "    validation_accuracy, yp = accuracy(X_validation, y_validation, w_n, b_n, n_layers, activation_f)\n",
        "    test_accuracy, ytp = accuracy(X_test, y_test, w_n, b_n, n_layers, activation_f)\n",
        "\n",
        "    cm = wandb.plot.confusion_matrix(y_true=y_test, preds=ytp, class_names=u_l)\n",
        "    wandb.log({\"conf_mat\": cm})\n",
        "\n",
        "\n",
        "\n",
        "    wandb.log({\"train_acc\": train_accuracy, \\\n",
        "               \"validation_acc\": validation_accuracy, \\\n",
        "                \"test_acc\": test_accuracy, \\\n",
        "                })\n",
        "    # data = [[x, y] for (x, y) in zip(xxnn, trac)]\n",
        "    # table = wandb.Table(data=data, columns = [\"x\", \"y\"])\n",
        "    # wandb.log({\"my_custom_plot_id\" : wandb.plot.line(table,\n",
        "    #                              \"x\", \"y\", title=\"train_accuracy_steps\")})\n",
        "    # data2 = [[x, y] for (x, y) in zip(xxnn, vaac)]\n",
        "    # table2 = wandb.Table(data=data2, columns = [\"x\", \"y\"])\n",
        "    # wandb.log({\"my_custom_plot_id1\" : wandb.plot.line(table2,\n",
        "    #                              \"x\", \"y\", title=\"validation_accuracy_steps\")})\n",
        "    # data3 = [[x, y] for (x, y) in zip(xxnn, teac)]\n",
        "    # table3 = wandb.Table(data=data3, columns = [\"x\", \"y\"])\n",
        "    # wandb.log({\"my_custom_plot_id2\" : wandb.plot.line(table3,\n",
        "    #                              \"x\", \"y\", title=\"test_accuracy_steps\")})\n",
        "\n",
        "    print(\"accu: \", validation_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####################################################################\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project = \"Assignment1\")\n",
        "wandb.agent(sweep_id, function = train_nn, count=3)\n",
        "wandb.finish()\n",
        "\n",
        "#train_nn()\n",
        "#wandb.finish()\n"
      ],
      "metadata": {
        "id": "zmIG1TpenQtk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}