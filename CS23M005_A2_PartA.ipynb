{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMw9AGkGC6DnzRbyr38F5YY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CS23M005/Assignment2_PartA/blob/main/CS23M005_A2_PartA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "EelbKOQZna8h"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTx13mkhacMR",
        "outputId": "10e974cf-336d-488a-c2db-64d6b9612f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/wandb_datasets/nature_12K.zip -O nature_12K.zip\n",
        "!unzip -q nature_12K.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Egc4oQ-l-WVF",
        "outputId": "af0d59e9-ca9c-4c39-e1be-d0e2789f5feb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-03 17:19:54--  https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.201.207, 74.125.69.207, 64.233.181.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.201.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3816687935 (3.6G) [application/zip]\n",
            "Saving to: ‘nature_12K.zip’\n",
            "\n",
            "nature_12K.zip      100%[===================>]   3.55G  73.2MB/s    in 49s     \n",
            "\n",
            "2024-04-03 17:20:43 (74.5 MB/s) - ‘nature_12K.zip’ saved [3816687935/3816687935]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm nature_12K.zip"
      ],
      "metadata": {
        "id": "ZMvyUFo_-lUi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import torchvision\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,),(0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root='inaturalist_12K/train',transform=transform)\n",
        "train_dataset,val_dataset = torch.utils.data.random_split(train_dataset,[8000,1999])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "    batch_size =32,shuffle = True,num_workers=2,pin_memory=True)\n",
        "\n",
        "test_data = torch.utils.data.DataLoader(\n",
        "    datasets.ImageFolder(root='inaturalist_12K/val',transform=transform),batch_size = 32,shuffle = True,num_workers=2,pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "5XKRsVXs9zd2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, input_channel, output_size, num_filters, filter_size, activation_fun, filter_config, maxPoolStride, stride, neurons):\n",
        "        super(ConvNet,self).__init__()\n",
        "        self.k = filter_size\n",
        "        m = self.getM()\n",
        "        self.s = stride\n",
        "        w, h = 256,256\n",
        "\n",
        "        self.conv1 = nn.Conv2d(input_channel, m[0], self.k, self.s)\n",
        "        w,h = self.getWH(w,h)\n",
        "        self.pool1 = nn.MaxPool2d(self.k,self.s)\n",
        "        w,h = self.getWH(w,h)\n",
        "        self.conv2 = nn.Conv2d(m[0], m[1], self.k, self.s)\n",
        "        w,h = self.getWH(w,h)\n",
        "        self.pool2 = nn.MaxPool2d(self.k, self.s)\n",
        "        w,h = self.getWH(w,h)\n",
        "        self.conv3 = nn.Conv2d(m[1], m[2], self.k, self.s)\n",
        "        w,h = self.getWH(w,h)\n",
        "        self.pool3 = nn.MaxPool2d(self.k, self.s)\n",
        "        w,h = self.getWH(w,h)\n",
        "        self.conv4 = nn.Conv2d(m[2], m[3], self.k, self.s)\n",
        "        w,h = self.getWH(w,h)\n",
        "        self.pool4 = nn.MaxPool2d(self.k, self.s)\n",
        "        w,h = self.getWH(w,h)\n",
        "        self.conv5 = nn.Conv2d(m[3], m[4], self.k, self.s)\n",
        "        w,h = self.getWH(w,h)\n",
        "        self.pool5 = nn.MaxPool2d(self.k, self.s)\n",
        "        w,h = self.getWH(w,h)\n",
        "        self.fc1 = nn.Linear(m[4]*w*h, neurons)\n",
        "        self.fc2 = nn.Linear(neurons,10)\n",
        "\n",
        "\n",
        "\n",
        "    def getWH(self, w, h):\n",
        "      return (math.floor(((w-self.k)/self.s)+1),math.floor(((w-self.k)/self.s)+1))\n",
        "\n",
        "    def getM(self):\n",
        "        m = []\n",
        "        if(filter_config == \"double\"):\n",
        "          for i in range(1,6):\n",
        "            m.append((int)(2**(i)*num_filters))\n",
        "        elif(filter_config == \"same\"):\n",
        "          for i in range(1,6):\n",
        "            m.append(int(num_filters))\n",
        "        elif(filter_config == \"half\"):\n",
        "          for i in range(1,6):\n",
        "            m.append((int)(num_filters/(2**(i))))\n",
        "        return m\n",
        "\n",
        "    def getActivation_fn(self):\n",
        "        if(activation_fun == \"relu\"):\n",
        "            activation_fn = F.relu\n",
        "        elif(activation_fun == \"gelu\"):\n",
        "            activation_fn = F.gelu\n",
        "        elif(activation_fun == \"silu\"):\n",
        "            activation_fn = F.silu\n",
        "        elif(activation_fun==\"mish\"):\n",
        "            activation_fn = F.mish\n",
        "        return activation_fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        activation_fn = self.getActivation_fn()\n",
        "        x = activation_fn(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = activation_fn(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = activation_fn(self.conv3(x))\n",
        "        x = self.pool3(x)\n",
        "        x = activation_fn(self.conv4(x))\n",
        "        x = self.pool4(x)\n",
        "        x = activation_fn(self.conv5(x))\n",
        "        x = self.pool5(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = activation_fn(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "x = torch.randn((1,3,256,256))\n",
        "input_channel=3\n",
        "output_size=10\n",
        "num_filters=32\n",
        "filter_size=3\n",
        "activation_fun = \"relu\"\n",
        "filter_config = \"same\"\n",
        "maxPoolStride = 2\n",
        "stride = 1\n",
        "neurons= 128\n",
        "model1 = ConvNet(input_channel, output_size, num_filters, filter_size, activation_fun, filter_config, maxPoolStride, stride, neurons).to(device)\n",
        "y = model1(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B35MKKVdbc_C",
        "outputId": "87bec709-5196-4b65-987e-582ce8194e46"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1188, -0.0448,  0.0622, -0.0530, -0.0801, -0.0069, -0.0533, -0.0200,\n",
            "          0.0986, -0.0430]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-IJUyJqmP-B"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self, input_channel, output_size, num_filters, filter_size, activation_fun, filter_config):\n",
        "    super(ConvNet,self).__init__()\n",
        "    self.k = filter_size\n",
        "\n",
        "    if(filter_config == \"double\"):\n",
        "      self.m = 2*num_filters\n",
        "    elif(filter_config == \"half\"):\n",
        "      self.m = num_filters/2\n",
        "    elif(filter_config == \"same\"):\n",
        "      self.m = num_filters\n",
        "\n",
        "    if(activation_fun==\"ReLU\"):\n",
        "      self.layer2 = nn.ReLU()\n",
        "    elif(activation_fun==\"GELU\"):\n",
        "      self.layer2 = nn.GELU()\n",
        "    elif(activation_fun == \"SiLU\"):\n",
        "      self.layer2 = nn.SiLU()\n",
        "    elif(activation_fun == \"Mish\"):\n",
        "      self.layer2 = nn.Mish()\n",
        "\n",
        "    self.dropout1 = nn.Dropout2d(0.25)\n",
        "    self.dropout2 = nn.Dropout2d(0.5)\n",
        "    self.fc1 = nn.Linear(9216, 128)\n",
        "    self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    self.input_layer1 = nn.Conv2d(input_channel, self.m, self.k, 1)\n",
        "    self.layer3 = nn.MaxPool2d(2,2)\n",
        "    self.layer1 = nn.Conv2d(self.m, self.m, self.k)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.input_layer1(x)\n",
        "    x=  self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = nn.Flatten(x)\n",
        "    return x\n",
        "    #print(model)\n",
        "\n",
        "x = torch.randn((10,3,28,28))\n",
        "input_channel=3\n",
        "output_size=10\n",
        "num_filters=32\n",
        "filter_size=3\n",
        "activation_fun = \"ReLU\"\n",
        "filter_config = \"same\"\n",
        "model1 = ConvNet(input_channel, output_size, num_filters, filter_size, activation_fun, filter_config)\n",
        "y = model1(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-BYEUgA1z0O",
        "outputId": "89d258e6-1ddd-4ecd-bd0b-a4349ce57540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flatten(\n",
            "  start_dim=tensor([[[[0.0141, 0.5200, 0.4232,  ..., 0.2180, 0.6843, 0.6098],\n",
            "            [0.3145, 1.0824, 1.1824,  ..., 0.3572, 0.6627, 0.4583],\n",
            "            [0.2714, 0.3871, 0.8526,  ..., 0.8004, 0.4321, 0.7791],\n",
            "            ...,\n",
            "            [0.6750, 0.0000, 0.4083,  ..., 0.8292, 0.5178, 0.7919],\n",
            "            [1.0533, 0.1760, 0.3061,  ..., 1.6953, 0.8528, 0.6622],\n",
            "            [0.6455, 0.3990, 0.4902,  ..., 0.3586, 1.2961, 0.3871]],\n",
            "  \n",
            "           [[0.7864, 0.9491, 0.9243,  ..., 0.7549, 0.3363, 1.2432],\n",
            "            [0.0795, 0.2885, 0.6865,  ..., 0.9556, 0.5447, 0.5059],\n",
            "            [0.9100, 1.4908, 1.2771,  ..., 0.7624, 1.0212, 0.8673],\n",
            "            ...,\n",
            "            [1.2515, 0.6684, 1.0149,  ..., 0.1129, 0.9377, 0.8494],\n",
            "            [0.0547, 0.4853, 0.8530,  ..., 0.6049, 1.2547, 0.4948],\n",
            "            [0.2218, 0.4209, 1.0938,  ..., 0.0026, 0.4970, 0.8769]],\n",
            "  \n",
            "           [[0.5306, 0.8761, 0.0849,  ..., 0.5633, 0.0000, 1.7139],\n",
            "            [0.7806, 0.4060, 1.1231,  ..., 0.6462, 0.5105, 0.6631],\n",
            "            [0.6211, 0.0796, 0.1907,  ..., 1.0109, 0.7640, 0.1803],\n",
            "            ...,\n",
            "            [1.3902, 1.2157, 0.4152,  ..., 1.7410, 0.0000, 0.9113],\n",
            "            [0.3933, 0.8186, 0.8151,  ..., 1.1533, 1.1787, 0.7452],\n",
            "            [0.4952, 1.1368, 0.1776,  ..., 0.8575, 0.2325, 0.7380]],\n",
            "  \n",
            "           ...,\n",
            "  \n",
            "           [[0.8876, 1.0173, 0.2002,  ..., 0.8456, 0.0000, 0.7632],\n",
            "            [0.2533, 0.0209, 0.7323,  ..., 0.8891, 1.0742, 0.0000],\n",
            "            [2.0737, 1.7051, 0.4358,  ..., 1.2103, 0.2234, 0.7685],\n",
            "            ...,\n",
            "            [1.0510, 0.6270, 0.5329,  ..., 0.4276, 1.1545, 0.8568],\n",
            "            [0.7422, 1.0324, 1.3320,  ..., 0.4026, 0.0000, 0.0000],\n",
            "            [0.6257, 1.1382, 1.4465,  ..., 0.1300, 0.3946, 0.1206]],\n",
            "  \n",
            "           [[1.0619, 0.5015, 1.2232,  ..., 1.3467, 1.2085, 1.1289],\n",
            "            [0.6959, 0.1370, 0.6158,  ..., 0.5711, 0.7800, 0.8202],\n",
            "            [1.0588, 0.6312, 1.1708,  ..., 0.6703, 0.7967, 0.9682],\n",
            "            ...,\n",
            "            [0.8372, 0.5075, 1.5843,  ..., 0.8156, 0.7469, 0.4835],\n",
            "            [0.4391, 1.0109, 0.0000,  ..., 0.4567, 0.9351, 0.6301],\n",
            "            [0.9651, 0.5393, 0.8790,  ..., 0.5848, 1.0876, 0.2495]],\n",
            "  \n",
            "           [[0.2124, 0.4392, 0.0861,  ..., 0.8116, 0.7360, 0.3173],\n",
            "            [0.2332, 0.3028, 0.9032,  ..., 0.7846, 0.8888, 0.4883],\n",
            "            [1.2185, 1.3597, 0.0000,  ..., 0.4399, 0.4881, 0.4336],\n",
            "            ...,\n",
            "            [1.2864, 0.1716, 0.3480,  ..., 0.8768, 0.3579, 0.0458],\n",
            "            [1.0304, 0.0000, 0.6229,  ..., 0.7138, 1.0970, 0.3292],\n",
            "            [0.0886, 0.7803, 1.7982,  ..., 0.9306, 0.8432, 0.7995]]],\n",
            "  \n",
            "  \n",
            "          [[[0.6717, 0.6483, 0.2252,  ..., 0.6958, 0.9594, 0.7326],\n",
            "            [0.2004, 0.5359, 0.0000,  ..., 0.4265, 0.3919, 0.9813],\n",
            "            [0.9672, 1.2120, 0.9353,  ..., 0.6050, 0.7135, 1.1503],\n",
            "            ...,\n",
            "            [0.0000, 0.9617, 1.0786,  ..., 0.4471, 0.0374, 1.0364],\n",
            "            [0.3033, 1.0416, 1.6469,  ..., 0.4105, 0.5793, 0.5554],\n",
            "            [0.0000, 1.2171, 0.2308,  ..., 1.2715, 0.2583, 0.1842]],\n",
            "  \n",
            "           [[1.6972, 0.4893, 0.8135,  ..., 0.3509, 0.8819, 0.9903],\n",
            "            [1.1745, 0.1817, 0.4052,  ..., 0.5918, 0.5602, 0.8239],\n",
            "            [1.3581, 1.0522, 0.7659,  ..., 0.8905, 0.6118, 1.2745],\n",
            "            ...,\n",
            "            [0.8232, 0.5497, 0.9620,  ..., 1.3133, 1.1673, 0.3828],\n",
            "            [0.9485, 0.4097, 1.5319,  ..., 0.2556, 0.0000, 0.2855],\n",
            "            [0.4682, 0.6881, 0.4611,  ..., 0.7890, 0.4463, 0.5065]],\n",
            "  \n",
            "           [[0.5316, 0.8601, 0.6901,  ..., 1.2420, 0.9683, 0.5213],\n",
            "            [1.2765, 1.0110, 0.1556,  ..., 1.5761, 0.3426, 0.7902],\n",
            "            [0.8649, 0.7064, 0.4215,  ..., 0.6144, 0.2258, 1.1607],\n",
            "            ...,\n",
            "            [1.1924, 1.0241, 0.0103,  ..., 0.4008, 2.0094, 0.5912],\n",
            "            [1.4870, 0.7847, 1.2279,  ..., 0.2703, 0.9574, 0.0420],\n",
            "            [1.0146, 0.0000, 0.6434,  ..., 1.0868, 0.1691, 0.4680]],\n",
            "  \n",
            "           ...,\n",
            "  \n",
            "           [[0.4832, 0.6657, 0.3972,  ..., 0.1088, 0.0000, 0.4679],\n",
            "            [0.2474, 0.7372, 1.3469,  ..., 0.7293, 0.8540, 0.6286],\n",
            "            [0.8921, 1.2216, 0.5727,  ..., 0.5635, 0.8686, 0.7623],\n",
            "            ...,\n",
            "            [1.2358, 1.4281, 1.1602,  ..., 1.0602, 0.6020, 0.8848],\n",
            "            [0.7768, 0.7091, 0.4870,  ..., 0.4717, 0.4008, 0.3244],\n",
            "            [0.7455, 1.4292, 1.0156,  ..., 0.9659, 1.1568, 1.0048]],\n",
            "  \n",
            "           [[0.7991, 0.8635, 0.3746,  ..., 0.4537, 0.5581, 0.3325],\n",
            "            [0.3820, 0.3712, 0.4198,  ..., 0.5615, 0.0000, 1.1745],\n",
            "            [0.5443, 0.3671, 0.6179,  ..., 0.0049, 1.1093, 1.5850],\n",
            "            ...,\n",
            "            [1.5078, 0.4350, 2.1390,  ..., 1.8244, 0.3084, 0.0629],\n",
            "            [0.8556, 0.9970, 0.0793,  ..., 0.3645, 0.6206, 0.8145],\n",
            "            [0.4869, 0.8799, 1.3962,  ..., 1.4337, 1.2541, 0.1414]],\n",
            "  \n",
            "           [[0.9548, 0.6363, 0.8709,  ..., 0.4277, 0.2557, 0.6046],\n",
            "            [0.5207, 0.9879, 0.7662,  ..., 0.2162, 0.6022, 0.4023],\n",
            "            [0.4001, 1.2600, 0.4131,  ..., 0.3259, 0.2649, 0.9044],\n",
            "            ...,\n",
            "            [0.6211, 0.5043, 0.2865,  ..., 0.2833, 0.2323, 0.8017],\n",
            "            [0.4958, 0.4558, 0.5953,  ..., 0.7195, 0.0000, 0.8118],\n",
            "            [0.8419, 0.3765, 0.9136,  ..., 0.4600, 0.8092, 0.3831]]],\n",
            "  \n",
            "  \n",
            "          [[[0.0000, 0.3474, 0.3076,  ..., 0.6858, 0.5872, 0.1878],\n",
            "            [0.4316, 0.6630, 0.7573,  ..., 0.5629, 0.5854, 0.7388],\n",
            "            [0.6250, 0.4306, 0.9166,  ..., 0.0136, 1.4263, 0.3418],\n",
            "            ...,\n",
            "            [0.1095, 0.0000, 0.6738,  ..., 0.5772, 0.6436, 0.6503],\n",
            "            [0.4596, 0.2893, 0.1120,  ..., 0.3049, 0.8122, 0.8660],\n",
            "            [0.0709, 0.5320, 0.0000,  ..., 0.3869, 0.4113, 0.5926]],\n",
            "  \n",
            "           [[0.8906, 0.3701, 0.5252,  ..., 0.3039, 0.5921, 0.4616],\n",
            "            [0.0787, 0.5261, 1.2374,  ..., 0.6111, 1.4950, 0.9380],\n",
            "            [0.7421, 0.2866, 0.7243,  ..., 1.2707, 0.6483, 1.8201],\n",
            "            ...,\n",
            "            [0.6489, 0.2283, 0.6259,  ..., 0.6817, 0.3473, 0.4433],\n",
            "            [1.1797, 0.8295, 0.2841,  ..., 1.0872, 0.5353, 0.4180],\n",
            "            [0.3458, 0.0245, 0.4090,  ..., 0.4551, 0.1366, 0.7994]],\n",
            "  \n",
            "           [[0.1180, 1.1021, 0.0000,  ..., 0.4935, 0.7279, 1.3427],\n",
            "            [1.3339, 1.3691, 0.1250,  ..., 0.7559, 1.3231, 1.3614],\n",
            "            [0.3645, 0.4216, 0.0980,  ..., 0.6417, 0.0189, 0.0658],\n",
            "            ...,\n",
            "            [0.0192, 0.0832, 1.5860,  ..., 0.3598, 0.4300, 1.1613],\n",
            "            [0.3553, 0.8664, 0.8261,  ..., 0.1187, 0.4694, 0.6447],\n",
            "            [0.0767, 0.4533, 0.8128,  ..., 0.1014, 0.8334, 0.7334]],\n",
            "  \n",
            "           ...,\n",
            "  \n",
            "           [[0.5802, 0.5626, 0.7122,  ..., 1.0822, 0.2972, 0.9503],\n",
            "            [0.6010, 0.4517, 0.4813,  ..., 0.7338, 0.8286, 1.1961],\n",
            "            [0.5198, 0.4271, 0.3466,  ..., 0.1451, 0.8841, 0.6491],\n",
            "            ...,\n",
            "            [0.1175, 0.6368, 0.2994,  ..., 0.0000, 0.9282, 0.3752],\n",
            "            [1.5550, 0.7329, 1.4253,  ..., 1.0160, 0.0000, 0.2618],\n",
            "            [0.0000, 0.6244, 0.2171,  ..., 1.3421, 1.5815, 0.5173]],\n",
            "  \n",
            "           [[0.9273, 0.7478, 0.4840,  ..., 0.3112, 1.3692, 0.7384],\n",
            "            [0.2688, 0.3676, 1.0649,  ..., 1.1556, 0.9135, 0.1019],\n",
            "            [0.8227, 0.4652, 0.2055,  ..., 0.7158, 0.6039, 0.3220],\n",
            "            ...,\n",
            "            [0.9897, 0.2049, 0.3102,  ..., 0.0263, 0.8394, 0.8667],\n",
            "            [1.1583, 1.1558, 0.7571,  ..., 0.7557, 0.5799, 0.3359],\n",
            "            [0.4665, 0.6073, 0.9841,  ..., 1.0100, 0.6987, 0.4937]],\n",
            "  \n",
            "           [[0.6056, 0.6443, 0.4720,  ..., 0.6000, 0.2747, 0.5008],\n",
            "            [0.9634, 0.3252, 0.0000,  ..., 0.2288, 0.4148, 0.8514],\n",
            "            [0.4561, 0.6878, 0.6104,  ..., 0.0670, 0.9612, 0.9362],\n",
            "            ...,\n",
            "            [0.5701, 0.6199, 0.2940,  ..., 0.5972, 1.0399, 0.3308],\n",
            "            [0.2306, 0.3460, 0.8299,  ..., 1.1172, 0.0569, 0.4360],\n",
            "            [0.3253, 0.1050, 0.3086,  ..., 0.3229, 1.0542, 0.5548]]],\n",
            "  \n",
            "  \n",
            "          ...,\n",
            "  \n",
            "  \n",
            "          [[[0.0000, 0.2486, 0.0000,  ..., 0.1467, 0.6568, 0.0000],\n",
            "            [0.0000, 0.9118, 0.3326,  ..., 0.0430, 0.7240, 0.2201],\n",
            "            [0.3202, 0.6854, 0.3536,  ..., 0.8916, 0.0000, 0.4080],\n",
            "            ...,\n",
            "            [0.4532, 0.8880, 0.7307,  ..., 1.0359, 0.4771, 1.0538],\n",
            "            [1.6012, 1.2904, 1.7332,  ..., 1.1974, 0.1040, 0.6017],\n",
            "            [0.7234, 1.4533, 0.6289,  ..., 0.4344, 0.2258, 1.6396]],\n",
            "  \n",
            "           [[0.5248, 0.6889, 1.1843,  ..., 0.6888, 1.2491, 0.6534],\n",
            "            [0.4184, 0.0000, 0.6269,  ..., 0.6563, 0.1267, 0.3059],\n",
            "            [0.9983, 0.3706, 0.6057,  ..., 0.8495, 0.8784, 0.7992],\n",
            "            ...,\n",
            "            [0.5290, 0.8556, 0.4840,  ..., 0.8778, 0.2483, 0.5123],\n",
            "            [1.0481, 0.5787, 0.2919,  ..., 0.7230, 1.0651, 0.3483],\n",
            "            [1.1484, 0.1755, 1.0529,  ..., 0.3738, 0.7994, 0.5098]],\n",
            "  \n",
            "           [[0.6457, 0.1918, 0.3450,  ..., 0.2321, 0.4290, 0.1989],\n",
            "            [0.5834, 1.1750, 1.1173,  ..., 0.7200, 0.5743, 0.3827],\n",
            "            [0.4160, 0.2976, 0.6988,  ..., 0.5783, 0.8848, 0.4683],\n",
            "            ...,\n",
            "            [0.6239, 0.2545, 1.2823,  ..., 1.5401, 0.2865, 1.1781],\n",
            "            [0.5306, 1.2392, 0.7890,  ..., 0.6753, 1.1181, 0.5896],\n",
            "            [0.9916, 0.1518, 0.6555,  ..., 0.4516, 0.0446, 0.6927]],\n",
            "  \n",
            "           ...,\n",
            "  \n",
            "           [[0.7557, 1.2360, 1.1853,  ..., 0.1447, 0.4777, 0.8382],\n",
            "            [0.8338, 1.2467, 0.2875,  ..., 0.9431, 0.1374, 0.2611],\n",
            "            [0.4883, 0.8601, 0.5821,  ..., 0.0000, 0.7089, 0.9586],\n",
            "            ...,\n",
            "            [0.6559, 1.2244, 1.4793,  ..., 0.1731, 1.0599, 0.4136],\n",
            "            [0.6538, 0.2171, 0.4585,  ..., 1.4253, 0.5927, 0.6442],\n",
            "            [0.3376, 0.9562, 0.9257,  ..., 0.1165, 0.4591, 0.4025]],\n",
            "  \n",
            "           [[0.8282, 0.3734, 1.0638,  ..., 0.5085, 0.0000, 1.1990],\n",
            "            [0.4209, 0.2748, 0.6946,  ..., 0.5805, 0.8086, 0.4854],\n",
            "            [0.3241, 0.5646, 0.4600,  ..., 0.6890, 0.5426, 0.9219],\n",
            "            ...,\n",
            "            [0.6586, 2.0812, 0.5119,  ..., 0.0000, 0.3798, 1.2839],\n",
            "            [0.0966, 0.4694, 0.3488,  ..., 0.5569, 1.3558, 0.6408],\n",
            "            [0.6674, 1.3794, 1.3101,  ..., 0.0352, 0.4121, 0.4748]],\n",
            "  \n",
            "           [[0.0421, 0.1804, 0.4625,  ..., 1.0082, 0.6568, 0.5539],\n",
            "            [0.9048, 0.6874, 0.0946,  ..., 0.5571, 0.3634, 0.3526],\n",
            "            [1.2809, 0.9060, 0.6544,  ..., 0.4761, 0.5902, 1.2380],\n",
            "            ...,\n",
            "            [0.8233, 0.7871, 0.8819,  ..., 0.6976, 0.6823, 0.4131],\n",
            "            [1.1173, 0.2955, 0.8874,  ..., 0.9304, 0.2441, 0.4472],\n",
            "            [0.8244, 1.3740, 0.9292,  ..., 0.4780, 0.5099, 0.9197]]],\n",
            "  \n",
            "  \n",
            "          [[[0.5099, 0.6344, 0.5448,  ..., 0.2975, 0.0000, 1.2095],\n",
            "            [1.0030, 0.7919, 1.0924,  ..., 0.4725, 0.9063, 0.6629],\n",
            "            [1.2899, 0.8868, 0.0471,  ..., 0.6695, 0.4639, 0.6974],\n",
            "            ...,\n",
            "            [0.6116, 0.5698, 0.6867,  ..., 0.7177, 0.4178, 0.0635],\n",
            "            [0.0000, 0.6143, 1.3465,  ..., 0.9999, 0.0975, 0.2170],\n",
            "            [0.0235, 0.7356, 0.0000,  ..., 0.6419, 0.5764, 1.5681]],\n",
            "  \n",
            "           [[0.6405, 1.7163, 0.7871,  ..., 0.8413, 0.4071, 0.9977],\n",
            "            [0.8987, 0.5879, 0.1461,  ..., 0.6227, 1.0847, 0.6526],\n",
            "            [0.9133, 0.1951, 0.3104,  ..., 0.5859, 0.9525, 0.5658],\n",
            "            ...,\n",
            "            [0.1197, 0.5695, 0.9804,  ..., 1.2282, 0.6963, 0.6205],\n",
            "            [0.1309, 0.3337, 0.7268,  ..., 0.2158, 1.0587, 0.6060],\n",
            "            [0.7182, 1.0960, 0.9863,  ..., 0.1279, 0.2261, 0.8338]],\n",
            "  \n",
            "           [[0.6890, 0.7053, 1.6838,  ..., 0.1696, 1.1308, 0.6985],\n",
            "            [0.9949, 0.7095, 0.3583,  ..., 0.6455, 0.6861, 1.2963],\n",
            "            [0.4522, 0.0611, 0.1965,  ..., 0.9548, 0.7434, 1.1855],\n",
            "            ...,\n",
            "            [0.6803, 1.1573, 1.8903,  ..., 0.6129, 1.4894, 0.5331],\n",
            "            [0.1396, 0.7973, 0.3455,  ..., 0.3840, 0.1548, 1.0181],\n",
            "            [1.2192, 0.5088, 0.0000,  ..., 0.6609, 1.2683, 1.2731]],\n",
            "  \n",
            "           ...,\n",
            "  \n",
            "           [[0.9091, 0.9719, 0.8937,  ..., 0.9874, 0.8587, 0.9638],\n",
            "            [0.0374, 0.8691, 1.2720,  ..., 0.4095, 0.3273, 0.5631],\n",
            "            [0.8105, 0.3378, 0.0000,  ..., 0.0065, 0.5097, 1.1439],\n",
            "            ...,\n",
            "            [0.8824, 0.5805, 0.6436,  ..., 0.5921, 0.4412, 1.0236],\n",
            "            [0.0000, 0.8040, 0.9903,  ..., 0.3220, 0.6178, 0.3386],\n",
            "            [1.0382, 0.8067, 0.8145,  ..., 0.4308, 1.5005, 1.3466]],\n",
            "  \n",
            "           [[1.0936, 1.6971, 1.5012,  ..., 0.7737, 1.0931, 0.6200],\n",
            "            [0.4614, 1.2425, 0.9181,  ..., 0.2187, 0.4649, 0.6894],\n",
            "            [0.5933, 0.6622, 0.7944,  ..., 0.4965, 0.8600, 0.0579],\n",
            "            ...,\n",
            "            [0.4123, 1.0418, 1.6904,  ..., 1.7034, 0.3753, 0.4414],\n",
            "            [0.2362, 1.7047, 0.8198,  ..., 0.3675, 1.2824, 0.7948],\n",
            "            [1.2236, 0.3082, 0.9876,  ..., 0.4420, 0.5744, 0.7888]],\n",
            "  \n",
            "           [[0.2750, 0.6962, 1.1310,  ..., 0.6502, 0.2396, 1.1558],\n",
            "            [0.8455, 0.7592, 1.1523,  ..., 0.4156, 1.0647, 0.2461],\n",
            "            [0.6161, 0.4009, 0.3746,  ..., 0.5494, 0.5612, 0.8514],\n",
            "            ...,\n",
            "            [0.7774, 0.0000, 0.5152,  ..., 0.5909, 0.9142, 0.6237],\n",
            "            [0.4240, 0.7314, 1.0848,  ..., 0.0777, 0.7328, 0.7215],\n",
            "            [0.0222, 0.8672, 0.5242,  ..., 0.4218, 1.0546, 0.4290]]],\n",
            "  \n",
            "  \n",
            "          [[[1.3246, 0.0000, 0.7421,  ..., 1.1162, 1.1126, 1.2475],\n",
            "            [1.0182, 0.5084, 0.4289,  ..., 1.0276, 0.8945, 0.5019],\n",
            "            [0.7860, 0.0867, 0.1523,  ..., 0.0000, 0.2813, 0.9938],\n",
            "            ...,\n",
            "            [0.0000, 0.4014, 0.1614,  ..., 1.1663, 1.1708, 0.5791],\n",
            "            [0.9202, 0.2973, 0.1060,  ..., 1.2811, 0.0192, 0.4369],\n",
            "            [1.1407, 0.7577, 0.3362,  ..., 0.3548, 1.0760, 1.3427]],\n",
            "  \n",
            "           [[0.9987, 0.5581, 0.2731,  ..., 1.0415, 0.9147, 0.3803],\n",
            "            [0.6873, 0.7975, 0.2961,  ..., 0.9726, 0.4892, 1.3630],\n",
            "            [0.0884, 0.6896, 0.9024,  ..., 1.3474, 1.4662, 1.1304],\n",
            "            ...,\n",
            "            [0.2108, 1.1634, 0.4482,  ..., 0.4659, 0.5612, 0.1234],\n",
            "            [0.4428, 1.0555, 0.6614,  ..., 0.7926, 1.0642, 0.8945],\n",
            "            [0.6458, 0.6825, 0.8542,  ..., 0.5249, 0.4133, 0.5988]],\n",
            "  \n",
            "           [[0.2969, 0.0000, 0.6608,  ..., 1.3384, 1.5162, 0.6273],\n",
            "            [0.9453, 0.5075, 0.5156,  ..., 0.3238, 0.9974, 1.2024],\n",
            "            [0.7426, 0.9070, 0.3095,  ..., 1.2403, 1.5700, 0.8489],\n",
            "            ...,\n",
            "            [1.2142, 0.8951, 0.1572,  ..., 1.3445, 1.0480, 0.2908],\n",
            "            [0.9368, 0.7015, 0.2410,  ..., 0.4617, 0.7051, 0.7323],\n",
            "            [0.8362, 1.1297, 0.9645,  ..., 0.1087, 0.8131, 1.7145]],\n",
            "  \n",
            "           ...,\n",
            "  \n",
            "           [[0.0677, 0.5417, 0.6144,  ..., 1.0408, 0.8229, 0.8657],\n",
            "            [0.7719, 0.7114, 0.1585,  ..., 0.0000, 0.5899, 0.4696],\n",
            "            [0.7427, 0.5714, 0.5970,  ..., 0.6608, 1.0896, 0.2023],\n",
            "            ...,\n",
            "            [1.2069, 0.6406, 0.0348,  ..., 1.5658, 1.3494, 0.9745],\n",
            "            [0.7447, 1.2035, 0.9490,  ..., 0.6208, 0.2907, 0.2334],\n",
            "            [0.3815, 0.9187, 1.1786,  ..., 0.7865, 1.0082, 0.3763]],\n",
            "  \n",
            "           [[0.2306, 1.1329, 1.2080,  ..., 0.6766, 0.4807, 0.0000],\n",
            "            [1.1701, 1.2230, 0.2934,  ..., 0.6254, 0.8685, 0.7320],\n",
            "            [1.0295, 0.0000, 0.5286,  ..., 1.0797, 1.4785, 0.7856],\n",
            "            ...,\n",
            "            [0.7853, 0.0000, 0.5865,  ..., 1.0420, 0.6433, 0.8845],\n",
            "            [0.3764, 1.5133, 0.6546,  ..., 0.5647, 1.8227, 0.2141],\n",
            "            [0.4450, 0.9261, 1.2223,  ..., 0.5313, 1.1943, 1.3196]],\n",
            "  \n",
            "           [[1.4736, 0.4129, 0.8331,  ..., 0.6977, 1.3875, 0.6156],\n",
            "            [0.5817, 0.4784, 0.3234,  ..., 0.4565, 0.7392, 0.5216],\n",
            "            [0.9763, 0.5518, 0.1548,  ..., 0.4851, 0.1911, 0.9042],\n",
            "            ...,\n",
            "            [0.2277, 0.6787, 0.3840,  ..., 0.6845, 1.6166, 0.4514],\n",
            "            [0.2598, 1.1421, 0.7958,  ..., 0.0065, 1.0329, 0.4979],\n",
            "            [0.5992, 0.4156, 0.8040,  ..., 0.4155, 0.5205, 0.9055]]]],\n",
            "         grad_fn=<MaxPool2DWithIndicesBackward0>), end_dim=-1\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = 'cuda:0'\n",
        "inputs = [torch.zeros((1, 1, 2048, 2048)).to(device)]*2\n",
        "weight = torch.ones((48, 1, 501, 501), dtype=torch.float32).to(device)\n",
        "for (i, input) in enumerate(inputs):\n",
        "    out = F.conv2d(input, weight)\n",
        "    print(out.shape)\n",
        "    print(torch.argmax(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfp4S9DqzG27",
        "outputId": "ee121d45-d757-41ab-c2be-605b9b8f87f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 48, 1548, 1548])\n",
            "tensor(0, device='cuda:0')\n",
            "torch.Size([1, 48, 1548, 1548])\n",
            "tensor(0, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "      self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "      self.dropout1 = nn.Dropout2d(0.25)\n",
        "      self.dropout2 = nn.Dropout2d(0.5)\n",
        "      self.fc1 = nn.Linear(9216, 128)\n",
        "      self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    # x represents our data\n",
        "    def forward(self, x):\n",
        "      # Pass data through conv1\n",
        "      x = self.conv1(x)\n",
        "      # Use the rectified-linear activation function over x\n",
        "      x = F.relu(x)\n",
        "\n",
        "      x = self.conv2(x)\n",
        "      x = F.relu(x)\n",
        "\n",
        "      # Run max pooling over x\n",
        "      x = F.max_pool2d(x, 2)\n",
        "      # Pass data through dropout1\n",
        "      x = self.dropout1(x)\n",
        "      # Flatten x with start_dim=1\n",
        "      x = torch.flatten(x, 1)\n",
        "      # Pass data through ``fc1``\n",
        "      x = self.fc1(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.dropout2(x)\n",
        "      x = self.fc2(x)\n",
        "\n",
        "      # Apply softmax to x\n",
        "      output = F.log_softmax(x, dim=1)\n",
        "      return output"
      ],
      "metadata": {
        "id": "aXxkWCkbz_D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Equates to one random 28x28 image\n",
        "random_data = torch.rand((1, 1, 28, 28))\n",
        "\n",
        "my_nn = Net()\n",
        "result = my_nn(random_data)\n",
        "print (result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXlz0LuV0BIz",
        "outputId": "ce73fb29-d999-43af-8b5c-8538855e961e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.3844, -2.3163, -2.1654, -2.2708, -2.3411, -2.3325, -2.2021, -2.3257,\n",
            "         -2.3507, -2.3600]], grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1347: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'[{epoch + 1}] loss: {running_loss / n_total_steps:.3f}')\n",
        "\n",
        "print('Finished Training')\n",
        "PATH = './cnn.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "l3Jrw4hvaaTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_nn():\n",
        "    config_defaults = {\n",
        "            \"num_epochs\": 3, \\\n",
        "            \"num_hidden_layers\":  4, \\\n",
        "            \"size_hidden_layer\": 128, \\\n",
        "            \"learning_rate\": 1e-3, \\\n",
        "            \"optimizer\": \"rmsprop\", \\\n",
        "            \"batch_size\": 32, \\\n",
        "            \"weight_init\": \"xavier\" , \\\n",
        "            \"activation\": \"tanh\", \\\n",
        "            \"loss\": \"crossentropy\", \\\n",
        "            \"reg_lambda\": 0.001, \\\n",
        "    }\n",
        "    wandb.init(config = config_defaults, project = \"Assignment1\")\n",
        "    config = wandb.config\n",
        "    print(config.optimizer)\n",
        "    wandb.run.name = \"e_{}_hl_{}_opt_{}_bs_{}_init_{}_ac_{}_loss_{}\".format(config.num_epochs,\\\n",
        "                                                                  config.num_hidden_layers,\\\n",
        "                                                                  config.size_hidden_layer,\\\n",
        "                                                                  config.learning_rate,\\\n",
        "                                                                  config.optimizer,\\\n",
        "                                                                  config.batch_size,\\\n",
        "                                                                  config.weight_init,\\\n",
        "                                                                  config.activation,\\\n",
        "                                                                  config.loss,\\\n",
        "                                                                  config.reg_lambda)\n",
        "\n",
        "    seed(42)\n",
        "    n_inputs = 784\n",
        "    n_layers = config.num_hidden_layers\n",
        "    n_neurons = config.size_hidden_layer\n",
        "    n_outputs = 10\n",
        "    init_mode = config.weight_init\n",
        "    w,b = init_nn(n_inputs, n_layers, n_neurons, n_outputs, init_mode)\n",
        "    w_old = w.copy()\n",
        "    b_old = b.copy()\n",
        "\n",
        "    eta = config.learning_rate\n",
        "    activation_f = config.activation\n",
        "    batch_size = config.batch_size\n",
        "    loss = config.loss\n",
        "    ud_lambda = config.reg_lambda\n",
        "    optimizer = config.optimizer\n",
        "\n",
        "    beta = 0.9\n",
        "    beta1 = 0.9\n",
        "    beta2 = 0.999\n",
        "\n",
        "    eps = 1e-9\n",
        "    max_epochs = config.num_epochs\n",
        "    momentum = 0.9\n",
        "\n",
        "\n",
        "\n",
        "    wandb.log({\"examples\": fig1})\n",
        "\n",
        "\n",
        "\n",
        "    if optimizer == \"sgd\":\n",
        "      w_n,b_n, a_n, h_n = sgd(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs)\n",
        "    elif optimizer == \"mgd\":\n",
        "      w_n,b_n, a_n, h_n = mgd(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, max_epochs, momentum)\n",
        "    elif optimizer == \"nag\":\n",
        "      w_n,b_n, a_n, h_n = nag(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, momentum)\n",
        "    elif optimizer == \"rmsprop\":\n",
        "      w_n,b_n, a_n, h_n = rmsprop(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, beta, eps)\n",
        "    elif optimizer == \"adam\":\n",
        "      w_n,b_n, a_n, h_n = adam(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, beta1, beta2, eps)\n",
        "    elif optimizer == \"nadam\":\n",
        "      w_n,b_n, a_n, h_n = nadam(n_inputs, n_layers, n_neurons, n_outputs, activation_f, batch_size, loss, ud_lambda, eta, init_mode, w, b, True, max_epochs, beta1, beta2, eps)\n",
        "\n",
        "\n",
        "    train_accuracy, yp = accuracy(X_train, y_train, w_n, b_n, n_layers, activation_f)\n",
        "    validation_accuracy, yp = accuracy(X_validation, y_validation, w_n, b_n, n_layers, activation_f)\n",
        "    test_accuracy, ytp = accuracy(X_test, y_test, w_n, b_n, n_layers, activation_f)\n",
        "\n",
        "    cm = wandb.plot.confusion_matrix(y_true=y_test, preds=ytp, class_names=u_l)\n",
        "    wandb.log({\"conf_mat\": cm})\n",
        "\n",
        "\n",
        "\n",
        "    wandb.log({\"train_acc\": train_accuracy, \\\n",
        "               \"validation_acc\": validation_accuracy, \\\n",
        "                \"test_acc\": test_accuracy, \\\n",
        "                })\n",
        "    # data = [[x, y] for (x, y) in zip(xxnn, trac)]\n",
        "    # table = wandb.Table(data=data, columns = [\"x\", \"y\"])\n",
        "    # wandb.log({\"my_custom_plot_id\" : wandb.plot.line(table,\n",
        "    #                              \"x\", \"y\", title=\"train_accuracy_steps\")})\n",
        "    # data2 = [[x, y] for (x, y) in zip(xxnn, vaac)]\n",
        "    # table2 = wandb.Table(data=data2, columns = [\"x\", \"y\"])\n",
        "    # wandb.log({\"my_custom_plot_id1\" : wandb.plot.line(table2,\n",
        "    #                              \"x\", \"y\", title=\"validation_accuracy_steps\")})\n",
        "    # data3 = [[x, y] for (x, y) in zip(xxnn, teac)]\n",
        "    # table3 = wandb.Table(data=data3, columns = [\"x\", \"y\"])\n",
        "    # wandb.log({\"my_custom_plot_id2\" : wandb.plot.line(table3,\n",
        "    #                              \"x\", \"y\", title=\"test_accuracy_steps\")})\n",
        "\n",
        "    print(\"accu: \", validation_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####################################################################\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project = \"Assignment1\")\n",
        "wandb.agent(sweep_id, function = train_nn, count=3)\n",
        "wandb.finish()\n",
        "\n",
        "#train_nn()\n",
        "#wandb.finish()\n"
      ],
      "metadata": {
        "id": "zmIG1TpenQtk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}